{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPhXfx8QNYf2/n37Z4/1ozG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/papapabi/torch-sandbox/blob/main/image_classification_preloaded.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image classification - Cifar10\n",
        "\n",
        "Cifar10 consists of 10 classes of 60_000 32x32 color images in 10 classes, with 6_000 images per class. There are 50_000 training images and 10_000 test images.\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class."
      ],
      "metadata": {
        "id": "doCVCGK1GJyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective"
      ],
      "metadata": {
        "id": "xQqOBvDQIDyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classify an image if it belongs to any of the 10 classes."
      ],
      "metadata": {
        "id": "tUS6nwrBIFI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting `torch.utils.data.Dataset`s from `torchvision`"
      ],
      "metadata": {
        "id": "JooTcxD6NKgy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1X6KXVIRIDLJ"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lgh6bNx-IDOd"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "vMdeY4ngE5un"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "original_train_ds = torchvision.datasets.CIFAR10(\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
        "test_ds = torchvision.datasets.CIFAR10(\"./data\", train=False, download=True, transform=transforms.ToTensor())\n",
        "x_sample, y_sample = original_train_ds[0]"
      ],
      "metadata": {
        "id": "w0VbgYHrGgCF"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E-jpqggcHHCz"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_sample.shape)\n",
        "print(y_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9oHwM40Go0H",
        "outputId": "dacca0f9-c067-4e58-c053-a0a4c7cce9cd"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 32, 32])\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# label mapping for a torchvision.dataset\n",
        "{i: label for i, label in enumerate(original_train_ds.classes)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Taq9hh30HH6F",
        "outputId": "e60596da-3f08-403f-d424-f4499e57811a"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'airplane',\n",
              " 1: 'automobile',\n",
              " 2: 'bird',\n",
              " 3: 'cat',\n",
              " 4: 'deer',\n",
              " 5: 'dog',\n",
              " 6: 'frog',\n",
              " 7: 'horse',\n",
              " 8: 'ship',\n",
              " 9: 'truck'}"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tHQ7wZ9GO06L"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting train into train/val\n",
        "\n",
        "To preserve the class ratios, we need to perform stratified sampling on the train set."
      ],
      "metadata": {
        "id": "Ag-73Mh5NP5k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dYX4hDXsPoqk"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Get the full indices for the train dataset\n",
        "original_train_indices = list(range(len(original_train_ds)))\n",
        "\n",
        "labels_for_stratification = original_train_ds.targets\n",
        "\n",
        "# Get the indices for train and val from the dataset\n",
        "train_indices, val_indices = train_test_split(original_train_indices, test_size=0.2, random_state=42, stratify=labels_for_stratification)"
      ],
      "metadata": {
        "id": "5E08IvPBHKIa"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "train_ds = torch.utils.data.Subset(dataset=original_train_ds, indices=train_indices)\n",
        "val_ds = torch.utils.data.Subset(dataset=original_train_ds, indices=val_indices)"
      ],
      "metadata": {
        "id": "q64U-r3JGu5m"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WJFDYGFch14z"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Split Dataset Info ---\")\n",
        "print(f\"Train dataset size: {len(train_ds)}\")\n",
        "print(f\"Validation dataset size: {len(val_ds)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mMSqt_8hmQY",
        "outputId": "b1d9a3d6-9319-4fc0-e634-65559153ba1d"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Split Dataset Info ---\n",
            "Train dataset size: 40000\n",
            "Validation dataset size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRkr1WA7g6lG",
        "outputId": "b3725301-d88b-42b5-a7aa-f87f2957c43f"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.2196, 0.2314, 0.2745,  ..., 0.5412, 0.5608, 0.6157],\n",
              "          [0.2157, 0.2471, 0.3176,  ..., 0.5294, 0.6706, 0.6392],\n",
              "          [0.2510, 0.3137, 0.3176,  ..., 0.6471, 0.7608, 0.5725],\n",
              "          ...,\n",
              "          [0.1765, 0.1961, 0.2627,  ..., 0.5294, 0.4431, 0.5020],\n",
              "          [0.1843, 0.1843, 0.2510,  ..., 0.4392, 0.3961, 0.4314],\n",
              "          [0.1647, 0.2078, 0.2471,  ..., 0.4902, 0.4314, 0.3686]],\n",
              " \n",
              "         [[0.1961, 0.2000, 0.2549,  ..., 0.6039, 0.6196, 0.6588],\n",
              "          [0.1961, 0.2275, 0.3020,  ..., 0.5843, 0.7333, 0.6941],\n",
              "          [0.2431, 0.2980, 0.3059,  ..., 0.7020, 0.8275, 0.6235],\n",
              "          ...,\n",
              "          [0.1608, 0.1804, 0.2471,  ..., 0.5294, 0.4431, 0.4980],\n",
              "          [0.1765, 0.1765, 0.2431,  ..., 0.4157, 0.3725, 0.4078],\n",
              "          [0.1647, 0.2039, 0.2353,  ..., 0.4471, 0.3961, 0.3412]],\n",
              " \n",
              "         [[0.1647, 0.1608, 0.2000,  ..., 0.5804, 0.6000, 0.6471],\n",
              "          [0.1490, 0.1843, 0.2549,  ..., 0.5569, 0.7176, 0.6941],\n",
              "          [0.1843, 0.2627, 0.2706,  ..., 0.6667, 0.8118, 0.6353],\n",
              "          ...,\n",
              "          [0.1333, 0.1451, 0.2039,  ..., 0.4431, 0.3569, 0.4157],\n",
              "          [0.1412, 0.1373, 0.1961,  ..., 0.3294, 0.2863, 0.3255],\n",
              "          [0.1333, 0.1725, 0.1961,  ..., 0.3569, 0.3020, 0.2431]]]),\n",
              " 6)"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds[0][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAPJgopzQZOA",
        "outputId": "2b2ea3b2-1205-4c8f-f27f-542a05fb931e"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "1bAQLyKdjGJ9",
        "outputId": "c0a91cbf-c3d3-4a09-8a3b-abfa54ba212e"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataset.Subset"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torch.utils.data.dataset.Subset</b><br/>def __init__(dataset: Dataset[_T_co], indices: Sequence[int]) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py</a>Subset of a dataset at specified indices.\n",
              "\n",
              "Args:\n",
              "    dataset (Dataset): The whole Dataset\n",
              "    indices (sequence): Indices in the whole set selected for subset</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 393);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verify stratified sampling"
      ],
      "metadata": {
        "id": "sJaqgTnCQhPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- Verification of Stratified Sampling ---\n",
        "\n",
        "def get_label_distribution(dataset_subset):\n",
        "    \"\"\"\n",
        "    Calculates the distribution of labels within a torch.utils.data.Subset.\n",
        "    Assumes the original dataset (accessed via dataset_subset.dataset) has a .targets attribute.\n",
        "    \"\"\"\n",
        "    labels = [dataset_subset.dataset]\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    total = len(labels)\n",
        "    distribution = {\n",
        "        label: {'count': count, 'ratio': count / total}\n",
        "        for label, count in zip(unique, counts)\n",
        "    }\n",
        "    return distribution, total\n",
        "\n",
        "print(\"\\n--- Label Distributions ---\")\n",
        "\n",
        "\n",
        "# 2. Train Dataset Distribution\n",
        "train_dist, train_total = get_label_distribution(train_ds)\n",
        "print(\"\\nTrain Dataset:\")\n",
        "for label, info in train_dist.items():\n",
        "    print(f\"  Class {label}: Count = {info['count']}, Ratio = {info['ratio']:.4f}\")\n",
        "\n",
        "# 3. Validation Dataset Distribution\n",
        "val_dist, val_total = get_label_distribution(val_ds)\n",
        "print(\"\\nValidation Dataset:\")\n",
        "for label, info in val_dist.items():\n",
        "    print(f\"  Class {label}: Count = {info['count']}, Ratio = {info['ratio']:.4f}\")\n",
        "\n",
        "# --- Compare Ratios ---\n",
        "print(\"\\n--- Ratio Comparison (Train vs. Validation vs. Original) ---\")\n",
        "all_labels = sorted(set(original_dist.keys()).union(train_dist.keys()).union(val_dist.keys()))\n",
        "\n",
        "for label in all_labels:\n",
        "    original_ratio = original_dist.get(label, {}).get('ratio', 0)\n",
        "    train_ratio = train_dist.get(label, {}).get('ratio', 0)\n",
        "    val_ratio = val_dist.get(label, {}).get('ratio', 0)\n",
        "\n",
        "    print(f\"Class {label}: Original Ratio={original_ratio:.4f}, Train Ratio={train_ratio:.4f}, Val Ratio={val_ratio:.4f}\")\n",
        "\n",
        "    # Check for approximate equality (due to potential rounding or small sample sizes)\n",
        "    if not np.isclose(original_ratio, train_ratio, atol=1e-2) or \\\n",
        "       not np.isclose(original_ratio, val_ratio, atol=1e-2):\n",
        "        print(f\"  WARNING: Ratios for Class {label} are not closely matched. Stratification might be imperfect due to small class counts or very small overall dataset size.\")\n",
        "\n",
        "print(\"\\n--- Stratification Verification Complete ---\")\n",
        "print(\"If the ratios for each class are very similar across Original, Train, and Val datasets,\")\n",
        "print(\"then stratified sampling has worked correctly.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "9XyvLGxXQiMp",
        "outputId": "f4d9a562-973f-4e9a-8b2d-e33c672f7ea9"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Label Distributions ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 3 dimensions. The detected shape was (1, 50000, 2) + inhomogeneous part.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-141-760531058.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# 2. Train Dataset Distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrain_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_label_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTrain Dataset:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-141-760531058.py\u001b[0m in \u001b[0;36mget_label_distribution\u001b[0;34m(dataset_subset)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[1;32m     10\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdataset_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0munique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     distribution = {\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_arraysetops_impl.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \"\"\"\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         ret = _unique1d(ar, return_index, return_inverse, return_counts, \n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 3 dimensions. The detected shape was (1, 50000, 2) + inhomogeneous part."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting `torch.utils.data.DataLoaders`"
      ],
      "metadata": {
        "id": "AZ94FVhLmrl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "U8V32zYJQitw"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and evaluation code"
      ],
      "metadata": {
        "id": "7HkhWgsznQLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm.autonotebook import tqdm"
      ],
      "metadata": {
        "id": "3ou92j7LQml8"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections.abc import Mapping, Sequence\n",
        "\n",
        "def move(obj, device):\n",
        "    \"\"\"\n",
        "    Recursively moves PyTorch tensors and modules within a Python object to a specified device.\n",
        "\n",
        "    Args:\n",
        "        obj: The Python object to move to a device, or to move its contents to a device.\n",
        "             Can be a torch.Tensor, torch.nn.Module, list, tuple, set, dict, or other types.\n",
        "        device: The compute device (e.g., 'cpu', 'cuda:0') to move objects to.\n",
        "\n",
        "    Returns:\n",
        "        The object with its PyTorch components moved to the specified device.\n",
        "    \"\"\"\n",
        "    if isinstance(obj, torch.Tensor):\n",
        "        return obj.to(device)\n",
        "    elif isinstance(obj, torch.nn.Module):\n",
        "        return obj.to(device)\n",
        "    elif isinstance(obj, Mapping):\n",
        "        return {k: move(v, device) for k, v in obj.items()}\n",
        "    elif isinstance(obj, Sequence) and not isinstance(obj, str): # Exclude strings as they are sequences of chars\n",
        "        return type(obj)(move(x, device) for x in obj)\n",
        "    else:\n",
        "        return obj"
      ],
      "metadata": {
        "id": "2LZ8da4xxWs8"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_epoch(model, optimizer, data_loader, loss_func, device, results, score_funcs, prefix=\"\", desc=None, scaler=None):\n",
        "    \"\"\"\n",
        "    Runs a single epoch of training or validation in PyTorch.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The PyTorch model to run for one epoch.\n",
        "        optimizer (torch.optim.Optimizer): The object that will update the weights of the network.\n",
        "                                           Pass None if in evaluation mode (no optimization needed).\n",
        "        data_loader (torch.utils.data.DataLoader): DataLoader object that returns tuples of (input, label) pairs.\n",
        "        loss_func (callable): The loss function that takes in two arguments (model outputs, labels)\n",
        "                              and returns a scalar loss.\n",
        "        device (torch.device or str): The compute location to perform training/evaluation (e.g., 'cpu', 'cuda:0').\n",
        "        results (dict): A dictionary to store epoch-wise metrics.\n",
        "        score_funcs (dict): A dictionary of scoring functions (name: function) to use to evaluate\n",
        "                            the performance of the model. Each function should take (y_true, y_pred).\n",
        "        prefix (str): A string to prefix to any scores placed into the `results` dictionary.\n",
        "                      Commonly 'train_' or 'val_'.\n",
        "        desc (str, optional): A description to use for the progress bar.\n",
        "        scaler: (sklearn.preprocessing.BaseScaler, optional): An optional scaler for target variables when scaling is applied during preprocessing.\n",
        "\n",
        "    Returns:\n",
        "        float: Time spent on the epoch in seconds.\n",
        "    \"\"\"\n",
        "    running_loss = []\n",
        "    y_true_all = []\n",
        "    y_pred_all = []\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # Set model to training or evaluation mode\n",
        "    # NOTE: Layers like Dropout and BatchNorm behave differently during training and evaluation.\n",
        "    # model.train() enables their training-specific behavior (e.g., dropout randomness, batch norm updating running stats),\n",
        "    # while model.eval() sets them to evaluation mode (e.g., no dropout, batch norm using learned running stats)\n",
        "    if optimizer is not None:\n",
        "        model.train()\n",
        "        # Enable anomaly detection for debugging during training\n",
        "        # with torch.autograd.set_detect_anomaly(True): # Uncomment for debugging\n",
        "        #     for inputs, labels in tqdm(data_loader, desc=desc, leave=False):\n",
        "        #         ...\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "\n",
        "    # Use torch.no_grad() for evaluation phase to save memory and speed up\n",
        "    # computations by not building the computational graph.\n",
        "    with torch.no_grad() if optimizer is None else torch.enable_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc=desc, leave=False):\n",
        "            # Move the batch to the device we are using.\n",
        "            inputs = move(inputs, device)\n",
        "            labels = move(labels, device)\n",
        "\n",
        "            # Forward pass\n",
        "            y_hat = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_func(y_hat, labels)\n",
        "\n",
        "            if model.training:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Use .item() for scalar loss to prevent memory leaks in the computational graph\n",
        "            running_loss.append(loss.item())\n",
        "\n",
        "            # Collect predictions and true labels for metric calculation\n",
        "            # It's generally good practice to keep these on CPU for metric calculation\n",
        "            # and to convert to numpy for compatibility with libraries like scikit-learn.\n",
        "            # Only process if score_funcs are provided and labels are tensors.\n",
        "            if len(score_funcs) > 0 and isinstance(labels, torch.Tensor):\n",
        "                # Detach from graph, move to CPU, convert to numpy\n",
        "                # For classification, often want raw logits for metrics,\n",
        "                # then apply argmax for accuracy.\n",
        "                # For regression, y_hat is already numerical.\n",
        "                labels_np = labels.detach().cpu().numpy()\n",
        "                y_hat_np = y_hat.detach().cpu().numpy()\n",
        "\n",
        "                # Extend with current batch's data\n",
        "                y_true_all.extend(labels_np.tolist())\n",
        "                y_pred_all.extend(y_hat_np.tolist())\n",
        "\n",
        "    # End training/evaluation epoch\n",
        "    end = time.time()\n",
        "\n",
        "    # Post-epoch metric calculations\n",
        "    y_pred_final = np.asarray(y_pred_all)\n",
        "    y_true_final = np.asarray(y_true_all) # Ensure y_true is also a numpy array\n",
        "\n",
        "    # Handle classification output (e.g., logits to class predictions)\n",
        "    # This logic assumes `y_pred_all` contains raw model outputs (logits or probabilities)\n",
        "    # and `y_true_all` contains integer class labels for classification, or continuous values for regression.\n",
        "    if y_pred_final.size > 0 and len(y_pred_final.shape) == 2 and y_pred_final.shape[1] > 1:\n",
        "        # Assuming multi-class classification where y_hat are logits/probabilities\n",
        "        y_pred_final_processed = np.argmax(y_pred_final, axis=1)\n",
        "    else:\n",
        "        # Assume regression or binary classification (where y_hat might be a single value)\n",
        "        y_pred_final_processed = y_pred_final\n",
        "\n",
        "    # Store results\n",
        "    results[prefix + \" loss\"].append(np.mean(running_loss))\n",
        "\n",
        "    if scaler:\n",
        "        y_true_final = scaler.inverse_transform(y_true_final)\n",
        "        y_pred_final_processed = scaler.inverse_transform(y_pred_final_processed)\n",
        "\n",
        "    for name, score_func in score_funcs.items():\n",
        "        try:\n",
        "            # Pass the processed predictions and true labels to score functions\n",
        "            results[prefix + \" \" + name].append(score_func(y_true_final, y_pred_final_processed))\n",
        "        except Exception as e: # Catch specific exception or general Exception\n",
        "            print(f\"Warning: Error calculating score '{name}': {e}. Appending NaN.\")\n",
        "            results[prefix + \" \" + name].append(float(\"NaN\"))\n",
        "\n",
        "    return end - start # time spent on epoch"
      ],
      "metadata": {
        "id": "5DmTUCsKxYj2"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from typing import Callable, Optional, Union, Dict, Any\n",
        "from collections import defaultdict\n",
        "\n",
        "def train_network(\n",
        "    model: nn.Module,\n",
        "    loss_func: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: Optional[DataLoader] = None,\n",
        "    score_funcs: Optional[Dict[str, Callable[[Any, Any], float]]] = None,\n",
        "    epochs: int = 50,\n",
        "    device: Union[str, torch.device] = \"cpu\",\n",
        "    checkpoint_file: Optional[str] = None,\n",
        "    lr_schedule: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n",
        "    optimizer: Optional[torch.optim.Optimizer] = None,\n",
        "    disable_tqdm: bool = False,\n",
        "    scaler=None,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Trains a PyTorch neural network.\n",
        "\n",
        "    Args:\n",
        "        model: The PyTorch model to train.\n",
        "        loss_func: The loss function that takes in model outputs and labels, and returns a scalar loss.\n",
        "        train_loader: PyTorch DataLoader for training data.\n",
        "        val_loader: Optional PyTorch DataLoader for validation data, evaluated after each epoch.\n",
        "        score_funcs: A dictionary of scoring functions (name: function) to evaluate model performance.\n",
        "                     Each function should take (y_true, y_pred).\n",
        "        epochs: The number of training epochs to perform.\n",
        "        device: The compute location (e.g., 'cpu', 'cuda:0') to perform training.\n",
        "        checkpoint_file: Optional path to a file for saving/loading model checkpoints.\n",
        "        lr_schedule: The learning rate scheduler. If provided, `optimizer` must also be provided.\n",
        "        optimizer: The optimizer used to update model parameters. If None, AdamW is used by default.\n",
        "        disable_tqdm: If True, disables the progress bar.\n",
        "        scaler: (sklearn.preprocessing.BaseScaler, optional): An optional scaler for target variables when scaling is applied during preprocessing.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing epoch-wise training, validation, and test results.\n",
        "    \"\"\"\n",
        "    if score_funcs is None:\n",
        "        score_funcs = {}\n",
        "\n",
        "    # Initialize results dictionary using defaultdict for cleaner appending\n",
        "    results: Dict[str, list] = defaultdict(list)\n",
        "\n",
        "    # Move model to the specified device\n",
        "    model.to(device)\n",
        "\n",
        "    if optimizer is None:\n",
        "        print(\"Optimizer not provided. Using AdamW as default.\")\n",
        "        optimizer = torch.optim.AdamW(model.parameters())\n",
        "\n",
        "    start_epoch = 0\n",
        "    # Load from checkpoint if specified and exists\n",
        "    if checkpoint_file and os.path.exists(checkpoint_file):\n",
        "        print(f\"Loading checkpoint from {checkpoint_file}...\")\n",
        "        checkpoint = torch.load(checkpoint_file, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        # Restore results if available\n",
        "        if 'results' in checkpoint:\n",
        "            # Ensure loaded results are compatible with defaultdict if needed\n",
        "            for k, v in checkpoint['results'].items():\n",
        "                results[k] = v\n",
        "        print(f\"Resuming training from epoch {start_epoch}.\")\n",
        "    elif checkpoint_file and not os.path.exists(checkpoint_file):\n",
        "        print(f\"Checkpoint file '{checkpoint_file}' not found. Starting training from scratch.\")\n",
        "\n",
        "    total_train_time = sum(results.get(\"total time\", [0.0])) # Accumulate time if resuming, initialize as float\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in tqdm(range(start_epoch, epochs), desc=\"Overall Epoch\", disable=disable_tqdm):\n",
        "        # --- Training Phase ---\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs} - Training...\")\n",
        "        current_epoch_train_time = run_epoch(\n",
        "            model, optimizer, train_loader, loss_func, device,\n",
        "            results, score_funcs, prefix=\"train\", desc=\"Training Batch\", scaler=scaler,\n",
        "        )\n",
        "        total_train_time += current_epoch_train_time\n",
        "\n",
        "        results[\"epoch\"].append(epoch)\n",
        "        results[\"total time\"].append(total_train_time)\n",
        "\n",
        "        # --- Validation Phase ---\n",
        "        if val_loader is not None:\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Validating...\")\n",
        "            # optimizer=None ensures run_epoch is in evaluation mode\n",
        "            run_epoch(\n",
        "                model, None, val_loader, loss_func, device,\n",
        "                results, score_funcs, prefix=\"val\", desc=\"Validation Batch\", scaler=scaler,\n",
        "            )\n",
        "\n",
        "        # --- Learning Rate Schedule Step ---\n",
        "        if lr_schedule is not None:\n",
        "            if isinstance(lr_schedule, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                # ReduceLROnPlateau needs a metric; typically validation loss\n",
        "                if val_loader is not None and \"val loss\" in results and len(results[\"val loss\"]) > 0:\n",
        "                    lr_schedule.step(results[\"val loss\"][-1])\n",
        "                else:\n",
        "                    print(\"Warning: ReduceLROnPlateau scheduler requires validation loss but 'val_loader' is None or 'val loss' not found. Skipping step.\")\n",
        "            else:\n",
        "                lr_schedule.step()\n",
        "            print(f\"Learning rate adjusted to: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        # --- Checkpointing ---\n",
        "        if checkpoint_file is not None:\n",
        "            try:\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'lr_scheduler_state_dict': lr_schedule.state_dict() if lr_schedule else None,\n",
        "                    'results': dict(results) # Convert defaultdict to dict for saving\n",
        "                }, checkpoint_file)\n",
        "                print(f\"Checkpoint saved to {checkpoint_file} at end of epoch {epoch+1}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving checkpoint: {e}\")\n",
        "\n",
        "    print(\"\\nTraining complete!\")\n",
        "    return pd.DataFrame.from_dict(results)"
      ],
      "metadata": {
        "id": "pgua-EhuxaRq"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_network(\n",
        "    model: nn.Module,\n",
        "    loss_func: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
        "    test_loader: DataLoader,\n",
        "    score_funcs: Optional[Dict[str, Callable[[Any, Any], float]]] = None,\n",
        "    device: Union[str, torch.device] = \"cpu\",\n",
        "    checkpoint_file: Optional[str] = None,\n",
        "    disable_tqdm: bool = False,\n",
        "    scaler=None,\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Evaluates a PyTorch neural network on a held-out test set.\n",
        "\n",
        "    Args:\n",
        "        model: The PyTorch model to evaluate.\n",
        "        loss_func: The loss function that takes in model outputs and labels, and returns a scalar loss.\n",
        "        test_loader: PyTorch DataLoader for the test data.\n",
        "        score_funcs: A dictionary of scoring functions (name: function) to evaluate model performance.\n",
        "                     Each function should take (y_true, y_pred).\n",
        "        device: The compute location (e.g., 'cpu', 'cuda:0') to perform evaluation.\n",
        "        checkpoint_file: Optional path to a file for loading a pre-trained model checkpoint.\n",
        "        disable_tqdm: If True, disables the progress bar.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float]: A dictionary containing the evaluation loss and scores.\n",
        "    \"\"\"\n",
        "    if score_funcs is None:\n",
        "        score_funcs = {}\n",
        "\n",
        "    # Move model to the specified device\n",
        "    model.to(device)\n",
        "\n",
        "    # Load from checkpoint if specified and exists\n",
        "    if checkpoint_file and os.path.exists(checkpoint_file):\n",
        "        print(f\"Loading model from checkpoint: {checkpoint_file} for evaluation...\")\n",
        "        checkpoint = torch.load(checkpoint_file, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        print(\"Model loaded successfully.\")\n",
        "    elif checkpoint_file and not os.path.exists(checkpoint_file):\n",
        "        print(f\"Warning: Checkpoint file '{checkpoint_file}' not found. Evaluating with current model state.\")\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = []\n",
        "    y_true_all = []\n",
        "    y_pred_all = []\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader, desc=\"Evaluating Test Set\", leave=False, disable=disable_tqdm):\n",
        "            inputs = move(inputs, device)\n",
        "            labels = move(labels, device)\n",
        "\n",
        "            y_hat = model(inputs)\n",
        "            loss = loss_func(y_hat, labels)\n",
        "            running_loss.append(loss.item())\n",
        "\n",
        "            if len(score_funcs) > 0 and isinstance(labels, torch.Tensor):\n",
        "                labels_np = labels.detach().cpu().numpy()\n",
        "                y_hat_np = y_hat.detach().cpu().numpy()\n",
        "\n",
        "                y_true_all.extend(labels_np.tolist())\n",
        "                y_pred_all.extend(y_hat_np.tolist())\n",
        "\n",
        "    end = time.time()\n",
        "    eval_time = end - start\n",
        "\n",
        "    # Post-evaluation metric calculations\n",
        "    y_pred_final = np.asarray(y_pred_all)\n",
        "    y_true_final = np.asarray(y_true_all)\n",
        "\n",
        "    if y_pred_final.size > 0 and len(y_pred_final.shape) == 2 and y_pred_final.shape[1] > 1:\n",
        "        y_pred_final_processed = np.argmax(y_pred_final, axis=1)\n",
        "    else:\n",
        "        y_pred_final_processed = y_pred_final\n",
        "\n",
        "    final_results: Dict[str, float] = {}\n",
        "    final_results[\"test loss\"] = np.mean(running_loss)\n",
        "    final_results[\"test_eval_time_seconds\"] = eval_time\n",
        "\n",
        "    if scaler:\n",
        "        y_true_final = scaler.inverse_transform(y_true_final)\n",
        "        y_pred_final_processed = scaler.inverse_transform(y_pred_final_processed)\n",
        "\n",
        "    for name, score_func in score_funcs.items():\n",
        "        try:\n",
        "            final_results[f\"test {name}\"] = score_func(y_true_final, y_pred_final_processed)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Error calculating score '{name}' during test evaluation: {e}. Setting to NaN.\")\n",
        "            final_results[f\"test {name}\"] = float(\"NaN\")\n",
        "\n",
        "    print(f\"\\nTest Evaluation Complete. Time: {eval_time:.2f} seconds.\")\n",
        "    print(\"Test Results:\")\n",
        "    for metric, value in final_results.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "    return final_results"
      ],
      "metadata": {
        "id": "pcQCeywgxd42"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the model"
      ],
      "metadata": {
        "id": "xUnGDCvpxpVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "D = 19 # we have 19 features\n",
        "n = 32\n",
        "target_classes = 10\n",
        "\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "score_funcs = {\"accuracy\": accuracy_score, \"precision\": precision_score, \"recall\": recall_score, \"f1\": f1_score}\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "pfv-gqJ9xfwr"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBottleNeck(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, leak_rate=0.1):\n",
        "        super().__init__()\n",
        "        # how much padding will our convolutional layers need to maintain input shape\n",
        "        pad = (kernel_size - 1) // 2\n",
        "        # the bottleneck should be smaller, so output/4 or input\n",
        "        bottleneck = max(out_channels//4, in_channels)\n",
        "\n",
        "        # Defines (3) sets of BN and convolution layers that we need\n",
        "        # For 1x1 convs, we use padding=0 because 1x1 will not change shape\n",
        "        self.F = torch.nn.Sequential(\n",
        "            torch.nn.BatchNorm2d(in_channels),\n",
        "            torch.nn.LeakyReLU(leak_rate),\n",
        "            torch.nn.Conv2d(in_channels, bottleneck, 1, padding=0),\n",
        "\n",
        "            torch.nn.BatchNorm2d(bottleneck),\n",
        "            torch.nn.LeakyReLU(leak_rate),\n",
        "            torch.nn.Conv2d(bottleneck, bottleneck, kernel_size, padding=pad),\n",
        "\n",
        "            torch.nn.BatchNorm2d(bottleneck),\n",
        "            torch.nn.LeakyReLU(leak_rate),\n",
        "            torch.nn.Conv2d(bottleneck, out_channels, 1, padding=0)\n",
        "        )\n",
        "\n",
        "        # by default, our shortcut is the identity function\n",
        "        self.shortcut = torch.nn.Identity()\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = torch.nn.Sequential(\n",
        "                torch.nn.Conv2d(in_channels, out_channels, 1, padding=0),\n",
        "                torch.nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.shortcut(x) + self.F(x)"
      ],
      "metadata": {
        "id": "geu_D4IEyC8A"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iswwww9bxsDt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}